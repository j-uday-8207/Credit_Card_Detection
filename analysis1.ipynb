{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from decisionTree import DecisionTree\n",
    "from randomForestt import RandomForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_credit = pd.read_csv(\"dataset/creditcard.zip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_df = df_credit.sample(n=10000, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = sample_df.drop([\"Class\"], axis=1).values #Setting the X to do the split\n",
    "y = sample_df[\"Class\"].values # transforming the values in array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_values_train, counts_train = np.unique(y_train, return_counts=True)\n",
    "print(\"Unique values in y_train:\", unique_values_train)\n",
    "print(\"Counts of unique values in y_train:\", counts_train)\n",
    "\n",
    "unique_values_test, counts_test = np.unique(y_test, return_counts=True)\n",
    "print(\"Unique values in y_test:\", unique_values_test)\n",
    "print(\"Counts of unique values in y_test:\", counts_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "smote = SMOTE(random_state=42)\n",
    "\n",
    "# Apply SMOTE to the training data\n",
    "X_resampled, y_resampled = smote.fit_resample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.metrics import precision_recall_curve, auc\n",
    " \n",
    "custom_tree = DecisionTree()\n",
    "\n",
    "param_grid = {\n",
    "    'max_depth': [None, 2, 5, 10],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "def aupr_score(y_true, y_score):\n",
    "    precision, recall, _ = precision_recall_curve(y_true, y_score)\n",
    "    aupr = auc(recall, precision)\n",
    "    return aupr\n",
    "\n",
    "scorer = make_scorer(aupr_score, greater_is_better=True)\n",
    "\n",
    "grid_search = GridSearchCV(custom_tree, param_grid, cv=5, scoring=scorer)\n",
    "grid_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_tree_params = grid_search.best_params_\n",
    "print(\"Best Decision Tree Parameters:\", best_tree_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = DecisionTree(max_depth=None, min_samples_split=10, min_samples_leaf=2)\n",
    "tree.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a single decision tree with the best parameters\n",
    "best_tree = DecisionTree(**best_tree_params)\n",
    "best_tree.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_single_tree = best_tree.predict(X_test)\n",
    "accuracy_single_tree = accuracy(y_test, y_pred_single_tree)\n",
    "print(\"Accuracy of Single Decision Tree:\", accuracy_single_tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_recall_curve, auc\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Make predictions on the test data\n",
    "y_pred = tree.predict(X_test)\n",
    "\n",
    "# Evaluate the classifier's performance\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Step 2: Calculate precision and recall\n",
    "precision, recall, thresholds = precision_recall_curve(y_test, y_pred)\n",
    "\n",
    "# Step 3: Plot the Precision-Recall Curve\n",
    "\n",
    "plt.plot(recall, precision, marker='.')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Precision-Recall Curve')\n",
    "\n",
    "# Step 4: Compute AUPR\n",
    "aupr = auc(recall, precision)\n",
    "print(\"Area Under Precision-Recall Curve (AUPR):\", aupr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Assuming y_true contains the true labels and y_pred contains the predicted labels\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "print(\"F1 Score:\", f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_classifier = RandomForest(n_estimators=50,\n",
    "                                        max_depth=None,\n",
    "                                        min_samples_split=10,\n",
    "                                        min_samples_leaf=2)\n",
    "rf_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = rf_classifier.predict(X_test)\n",
    "\n",
    "# Evaluate the classifier's performance\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Step 2: Calculate precision and recall\n",
    "precision, recall, thresholds = precision_recall_curve(y_test, y_pred)\n",
    "\n",
    "# Step 3: Plot the Precision-Recall Curve\n",
    "\n",
    "plt.plot(recall, precision, marker='.')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Precision-Recall Curve')\n",
    "\n",
    "# Step 4: Compute AUPR\n",
    "aupr = auc(recall, precision)\n",
    "print(\"Area Under Precision-Recall Curve (AUPR):\", aupr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming y_true contains the true labels and y_pred contains the predicted labels\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "print(\"F1 Score:\", f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import average_precision_score\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import precision_recall_curve, auc\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "df1 = df_credit.sample(10000)\n",
    "\n",
    "# Create copies of df1 for different experiments\n",
    "df2 = df1.copy()\n",
    "df3 = df1.copy()\n",
    "df4 = df1.drop(columns=['Class']).copy()\n",
    "\n",
    "# Function to classify based on threshold\n",
    "def classify_anomaly_score(score, threshold):\n",
    "    if score <= threshold:\n",
    "        return 1  # Fraud\n",
    "    else:\n",
    "        return 0  # Normal\n",
    "\n",
    "# Lists to store results\n",
    "no_of_trees_list = [50, 100, 150]\n",
    "sample_splits = [2, 5, 10]\n",
    "sample_leafs = [1, 2, 5]\n",
    "\n",
    "\n",
    "# Loop over different values of number of trees and sample size\n",
    "for no_of_trees in no_of_trees_list:\n",
    "    aupr_scores = []\n",
    "    aupr_scores_1 = []\n",
    "    f1_scores = [] \n",
    "    for sample_split in sample_splits:\n",
    "        for sample_leaf in sample_leafs:\n",
    "\n",
    "        \n",
    "            rf_classifier = RandomForest(n_estimators=no_of_trees,\n",
    "                                        max_depth=None,\n",
    "                                        min_samples_split=sample_split,\n",
    "                                        min_samples_leaf=sample_leaf)\n",
    "            rf_classifier.fit(X_train, y_train)\n",
    "\n",
    "            # Make predictions\n",
    "            y_pred = rf_classifier.predict(X_test)\n",
    "\n",
    "            aupr_score = auc(recall, precision)\n",
    "            aupr_scores.append(aupr_score)\n",
    "            \n",
    "            f1 = f1_score(y_test, y_pred)\n",
    "            f1_scores.append(f1)\n",
    "\n",
    "    # Reshape the AUPR scores for plotting\n",
    "    aupr_scores = np.array(aupr_scores).reshape(len(no_of_trees_list), len(sample_size_list))\n",
    "    f1_scores = np.array(f1_scores).reshape(len(no_of_trees_list), len(sample_size_list))\n",
    "\n",
    "    # Plot AUPR scores\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.imshow(aupr_scores, cmap='viridis', interpolation='nearest')\n",
    "    plt.title('AUPR Scores')\n",
    "    plt.xlabel('Sample Size')\n",
    "    plt.ylabel('Number of Trees')\n",
    "    plt.xticks(np.arange(len(sample_splits)), sample_splits)\n",
    "    plt.yticks(np.arange(len(sample_leafs)), sample_leafs)\n",
    "    plt.colorbar(label='AUPR Score')\n",
    "    plt.tight_layout()\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.imshow(f1_scores, cmap='viridis', interpolation='nearest')\n",
    "    plt.title('F1 Scores')\n",
    "    plt.xlabel('Sample Size')\n",
    "    plt.ylabel('Number of Trees')\n",
    "    plt.xticks(np.arange(len(sample_splits)), sample_splits)\n",
    "    plt.yticks(np.arange(len(sample_leafs)), sample_leafs)\n",
    "    plt.colorbar(label='F1 Score')\n",
    "    plt.tight_layout()\n",
    "\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
